# -*- coding: utf-8 -*-
"""
DGA家族域名
特征提取
归一化
"""
import re
import pickle
import math
import wordfreq
import operator
import string
import tld
import numpy as np
import pandas as pd
from configparser import ConfigParser
from collections import Counter, defaultdict
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
from sklearn.metrics import pairwise_distances
from sklearn.neighbors import NearestNeighbors
from sklearn.neighbors import KernelDensity
from sklearn.preprocessing import LabelEncoder
import umap
from sklearn.decomposition import KernelPCA
from sklearn.preprocessing import StandardScaler
from scipy.stats import norm
from matplotlib.colors import LinearSegmentedColormap, Normalize
# 读取配置设置
cp = ConfigParser()
cp.read('config.ini')
HMM_ADD = cp.get('files', 'hmm_add')
GIB_ADD = cp.get('files', 'gib_add')
GRAMFILE_ADD = cp.get('files', 'gramfile_add')
TLD_ADD = cp.get('files', 'tld_add')
DGA_DOMAINS = cp.get('files', 'dga_domains')
FEATURE_DIR = cp.get('files', 'feature_dir')
MODEL_DIR = cp.get('files', 'model_path')
# 顶级域名
DGATLD_LIST = ["cf", "recipes", "email", "ml", "gq", "fit", "cn", "ga", "rest", "tk"]
HMM_PROB_THRESHOLD = -120

# 全局变量
data_tsne_global = None
data_tsne_global2 = None
data_tsne_Unknown = None
data_umap_global = None
data_umap_global2 = None
# 从指定文件读取的顶级域名
TLD_LIST = list()
with open(TLD_ADD, 'r', encoding='utf8') as f:
    for i in f.readlines():
        TLD_LIST.append(i.strip().strip('.'))

ACCEPTED_CHARS = 'abcdefghijklmnopqrstuvwxyz '
POS = dict([(char, idx) for idx, char in enumerate(ACCEPTED_CHARS)])

# 从给定的URL中提取二级域名（SLD，Second-Level Domain）和顶级域名（TLD，Top-Level Domain）
def get_name(url):
    """
    用python自带库进行域名提取
    :param url: url
    :return: 二级域名，顶级域名
    """
    # 删除所有的标点符号
    url = url.strip(string.punctuation)
    try:
        TLD = tld.get_tld(url, as_object=True, fix_protocol=True)
        SLD = tld.get_tld(url, as_object=True, fix_protocol=True).domain

    except Exception as e:
        na_list = url.split(".")
        TLD = na_list[-1]
        SLD = na_list[-2]
    # 以字符串的形式返回提取到的TLD和SLD
    return str(TLD), str(SLD)

def wash_tld(dn):
    """
    将域名字符串中顶级域名去掉，剩余部分拼接成完整字符串
    :param dn: 原始域名
    :return: 拼接字符串
    """
    dn_list = dn.split('.')
    dn_list = list(set(dn_list).difference(set(TLD_LIST)))
    namestring = "".join(dn_list)
    return namestring

def cal_rep_cart(ns):
    """
    计算字符串中重复出现的字符个数
    :param SLD: 字符串
    :return: 重复字符个数
    """
    count = Counter(i for i in ns).most_common()
    sum_n = 0
    for letter, cnt in count:
        if cnt > 1:
            sum_n += 1
    return sum_n




def cal_ent_gni_cer(SLD):
    """
    计算香农熵, Gini值, 字符错误的分类
    :param url:
    :return:
    """
    f_len = float(len(SLD))
    count = Counter(i for i in SLD).most_common()
    # unigram frequency
    # 计算香农熵（Shannon Entropy）
    ent = -sum(float(j / f_len) * (math.log(float(j / f_len), 2)) for i, j in count)  # shannon entropy
    # 计算 Gini 值
    gni = 1 - sum(float(j / f_len) * float(j / f_len) for i, j in count)
    # 计算字符错误的分类（Character Error Rate），它是指出现频率最高的字符的频率与总字符数的比例
    cer = 1 - max(float(j/ f_len) for i, j in count)
    return ent, gni, cer


def cal_gram_med(SLD, n):
    """
    计算字符串n元频率中位数
    :param SLD: 字符串
    :param n: n
    :return:
    """
    if len(SLD) < n:
        return 0
    # 使用列表推导式生成长度为n的所有子字符串，并将其存储在名为grams的列表中
    grams = [SLD[i:i + n] for i in range(len(SLD) - n+1)]
    # 空列表fre，存储每个n元子字符串的频率。
    fre = list()
    # 遍历grams列表中的每个n元子字符串
    for s in grams:
        fre.append(wordfreq.zipf_frequency(s, 'en'))
    # 计算fre列表中频率的中位数
    return np.median(fre)


def cal_avg_gram_rank_sld(SLD,n): 
    """
    计算二级域名中所有n元字符组出现频次的平均值
    :param SLD: 字符串
    :param n: n
    :return:
    """
    if len(SLD) < n:
        return 0
    # 使用列表推导式生成长度为n的所有子字符串，并将其存储在名为grams的列表中
    grams = [SLD[i:i + n] for i in range(len(SLD) - n+1)]
    # 空列表fre，存储每个n元子字符串的频率。
    frequencies = [wordfreq.zipf_frequency(s, 'en') for s in grams]
    # 计算频率的平均值
    average_frequency = np.mean(frequencies)
    
    return average_frequency



def cal_std_gram_rank_sld(SLD,n):
    """
    计算二级域名中所有一元（1-gram）字符组出现频次的排序标准差
    :param SLD: 字符串
    :param n: n
    :return:
    """
    if len(SLD) < n:
        return 0
    
    # 使用列表推导式生成长度为 n 的所有子字符串，并将其存储在名为 grams 的列表中
    grams = [SLD[i:i + n] for i in range(len(SLD) - n + 1)]
    
    # 存储每个 n 元子字符串的频率
    frequencies = [wordfreq.zipf_frequency(s, 'en') for s in grams]
    
    # 计算频率的排序标准差
    std_deviation = np.std(frequencies)
    
    return std_deviation

def cal_hmm_prob(url):
    """
    计算成文概率, 结果越小越异常
    :param url:
    :return: 概率
    """
    hmm_dic = defaultdict(lambda: defaultdict(float))
    with open(HMM_ADD, 'r') as f:
        for line in f.readlines():
            key1, key2, value = line.rstrip().split('\t')  # key1 can be '' so rstrip() only
            value = float(value)
            hmm_dic[key1][key2] = value
    url = '^' + url.strip('.') + '$'
    gram2 = [url[i:i+2] for i in range(len(url)-1)]
    prob = hmm_dic[''][gram2[0]]

    for i in range(len(gram2)-1):
        prob *= hmm_dic[gram2[i]][gram2[i+1]]
    if prob < math.e ** HMM_PROB_THRESHOLD:
        prob = -999
    return prob

# 计算一个Gib标签，标识字符串是否正常
def cal_gib(SLD):
    """
    计算gib标签
    :param SLD:
    :return: 1: 正常 0: 异常
    """
    gib_model = pickle.load(open(GIB_ADD, 'rb'))
    mat = gib_model['mat']
    threshold = gib_model['thresh']

    log_prob = 0.0
    transition_ct = 0
    SLD = re.sub("[^a-z]", "", SLD)
    gram2 = [SLD[i:i + 2] for i in range(len(SLD) - 1)]
    for a, b in gram2:
        log_prob += mat[POS[a]][POS[b]]
        transition_ct += 1
    # The exponentiation translates from log probs to probs.
    prob = math.exp(log_prob / (transition_ct or 1))
    return int(prob > threshold)

# 加载一个 n 元排序字典
def load_gramdict():
    """
    加载n元排序字典
    :return: 字典
    """
    rank_dict = dict()
    with open(GRAMFILE_ADD, 'r') as f:
        for line in f:
            cat, gram, freq, rank = line.strip().split(',')
            rank_dict[gram] = int(rank)
    return rank_dict

#接受一个域名字符串 dn 作为输入，并返回一个包含25个特征的列表
def get_feature(dn):
    """
    
    :param url: 域名
    :return: 25维特征
    """
    TLD, SLD = get_name(dn)
    url = SLD + "." + TLD
    url_rm = re.sub(r"\.|_|-", "", url)
    TLD_rm = re.sub(r"\.|_|-", "", TLD)
    SLD_rm = re.sub(r"\.|_|-", "", SLD)

    # 1. 域名总长度
    domain_len = len(url)
    # 2. SLD长度
    sld_len = len(SLD)
    # 3. TLD长度
    tld_len = len(TLD)
    # 4. 域名不重复字符数
    uni_domain = len(set(url_rm))
    # 5. SLD不重复字符数
    uni_sld = len(set(SLD_rm))
    # 6. TLD不重复字符数
    uni_tld = len(set(TLD_rm))

    # 7. 是否包含某些恶意顶级域名 https://www.spamhaus.org/statistics/tlds/
    flag_dga = 0
    for t in DGATLD_LIST:
        if t in url:
            flag_dga = 1

    # 8. 是否以数字开头
    flag_dig = 0
    if re.match("[0-9]", url) != None:
        flag_dig = 1

    # 9. 特殊符号在SLD中占比
    sym = len(re.findall(r"\.|_|-", SLD)) / sld_len
    # 10. 十六进制字符在SLD中占比
    hex = len(re.findall(r"[0-9]|[a-f]", SLD)) / sld_len
    # 11. 数字在SLD中占比
    dig = len(re.findall(r"[0-9]", SLD)) // sld_len
    # 12. 元音字母在SLD中占比
    vow = len(re.findall(r"a|e|i|o|u", SLD)) / sld_len
    # 13. 辅音字母在SLD中占比
    con = len(re.findall(r"b|c|d|f|g|h|j|k|l|m|n|p|q|r|s|t|v|w|x|y|z", SLD)) / sld_len
    
    # 14. 重复字符在SLD不重复字符中占比
    rep_char_ratio = cal_rep_cart(SLD_rm) / uni_sld
    # 28. 重复字符占比, 二级域名中，出现频次大于1的字符数与二级域名总长的比值。
    rep_letter_sld = cal_rep_cart(SLD_rm) / sld_len
    # 15. 域名中连续辅音占比
    con_list = re.findall(r"[b|c|d|f|g|h|j|k|l|m|n|p|q|r|s|t|v|w|x|y|z]{2,}", url)
    con_len = [len(con) for con in con_list]
    cons_con_ratio = sum(con_len) / domain_len
    # 16. 域名中连续数字占比
    dig_list = re.findall(r"[0-9]{2,}", url)
    dig_len = [len(dig) for dig in dig_list]
    cons_dig_ratio = sum(dig_len) / domain_len
    # 17. SLD中由'-'分割的令牌数
    tokens_sld = len(SLD.split('-'))
    # 18. SLD中数字总数
    digits_sld = len(re.findall(r"[0-9]", SLD))
    # 19. SLD中字符的归一化熵值
    # 20. SLD的Gini值
    # 21. SLD中字符分类的错误
    ent, gni, cer = cal_ent_gni_cer(SLD)
    # 22. SLD中2元频次的中位数
    gram2_med = cal_gram_med(SLD, 2)
    # 23. SLD中3元频次的中位数
    gram3_med = cal_gram_med(SLD, 3)
    # 24. 重复SLD中2元频次中位数
    gram2_cmed = cal_gram_med(SLD + SLD, 2)
    # 25. 重复SLD中3元频次中位数
    gram3_cmed = cal_gram_med(SLD + SLD, 3)
    # 26. 域名的hmm成文概率
    hmm_prob = cal_hmm_prob(url)
    # 27. gib判断SLD是否成文
    sld_gib = cal_gib(SLD)
    # 29. 1 元字符频次排序平均值
    avg_gram1_rank_sld = cal_avg_gram_rank_sld(SLD,1)

    # 30. 2 元字符频次排序平均值
    avg_gram2_rank_sld = cal_avg_gram_rank_sld(SLD,2)
    # 31. 3 元字符频次排序平均值
    avg_gram3_rank_sld = cal_avg_gram_rank_sld(SLD,3)
    # 32. 1 元字符频次排序标准差
    std_gram1_rank_sld = cal_std_gram_rank_sld(SLD,1)
    # 33. 2 元字符频次排序标准差
    std_gram2_rank_sld = cal_std_gram_rank_sld(SLD,2)
    # 34. 3 元字符频次排序标准差
    std_gram3_rank_sld = cal_std_gram_rank_sld(SLD,3)

    feature = [domain_len, sld_len, tld_len, uni_domain, uni_sld, uni_tld, flag_dga, flag_dig, sym, hex, dig, vow,
               con, rep_char_ratio, rep_letter_sld, cons_con_ratio, cons_dig_ratio, tokens_sld, digits_sld, ent, gni, cer, gram2_med,
               gram3_med, gram2_cmed, gram3_cmed, hmm_prob, sld_gib,avg_gram1_rank_sld, avg_gram2_rank_sld,avg_gram3_rank_sld,std_gram1_rank_sld,std_gram2_rank_sld,std_gram3_rank_sld]
    return feature

"""
上述部分完成了域名的特征提取
"""

def feature_extraction(df):
    """
    特征提取, 归一化
    :param df:
    :return:
    """
    col = ["domain_name", "label", "domain_len", "sld_len", "tld_len", "uni_domain", 
           "uni_sld", "uni_tld","flag_dga", "flag_dig", "sym", "hex", "dig", "vow",
             "con", "rep_char_ratio", "rep_letter_sld","cons_con_ratio",
           "cons_dig_ratio", "tokens_sld", "digits_sld", "ent", 
           "gni", "cer", "gram2_med", "gram3_med", "gram2_cmed",
           "gram3_cmed", "hmm_prob", "sld_gib","avg_gram1_rank_sld", "avg_gram2_rank_sld","avg_gram3_rank_sld","std_gram1_rank_sld","std_gram2_rank_sld","std_gram3_rank_sld"]
    
    # 空列表，用于存储提取的特征
    fea_list = list()
    # 遍历了DataFrame中的每一行，通过 df.index 获取索引
    for ind in df.index:
        # 从DataFrame中获取当前行的数据，并将其转换为列表形式
        fea = df.loc[ind].tolist()
        if ind % 1000 == 0:
            print("{}...".format(ind))
        # 提取域名特征
        fea.extend(get_feature(df.at[ind, 0]))
        fea_list.append(fea)
    # 将提取的特征列表转换为一个新的DataFrame fea_df，并将列名设置为 col 中定义的名称
    fea_df = pd.DataFrame(fea_list, columns=col)

    return fea_df

def dataset_generation():
    """
    数据集归一化,
    :return:
    """
    # 读取DGA家族的域名数据，这些数据包含了域名和相应的标签信息
    # df = pd.read_csv(DGA_DOMAINS, header=None)
    df = pd.read_csv(DGA_DOMAINS, header=None)
    df_test = df.reset_index(drop=True)

    print("__________Generating New test Set__________")
    
    test_feature = feature_extraction(df_test)
    test_feature.to_csv(r"{}\2019_2020_4000test.csv".format(FEATURE_DIR), index=None)
    test_feature = test_feature.set_index(['domain_name', 'label'])

    # 标准化（归一化）特征数据
    standardScaler = StandardScaler()

    # 在测试集上拟合StandardScaler对象
    test_feature_values = test_feature.values
    standardScaler.fit(test_feature_values)
    
    # 归一化处理
    test_feature = pd.DataFrame(standardScaler.transform(test_feature), index=test_feature.index,
                                columns=test_feature.columns)
    test_feature = test_feature.reset_index()
    # 将已归一化的测试集特征数据保存为CSV文件
    test_feature.to_csv(r"{}\2019_2020_4000test.csv".format(FEATURE_DIR), index=None)
    pickle.dump(standardScaler, open(r"{}\standardscalar.pkl".format(MODEL_DIR), 'wb'))
    return

"""
t-SNE进行特征降维
"""
def traindataset_tSNE(csv_file_path):
    # 加载CSV文件
    # 读取训练集数据
    # global data_tsne  
    df = pd.read_csv(csv_file_path)
    
    # /home/dym/恶意域名检测/代码/web_dga/data/features/raw_test_features.csv
    # 提取特征列
    data = df.iloc[:, 2:]  # 假设前两列是标签和域名，所以从第3列开始提取特征
    # 假设标签存储在 'label' 列中
    labels = df["label"]

    # 创建一个t-SNE模型
    # n_components：指定要降维到的目标维度数。
    # random_state：这个参数用于控制随机性。
    # perplexity：高维空间中每个对象的有效邻居数量
    # distance metric：距离度量，'euclidean' 欧氏距离
    tsne = TSNE(n_components=2, perplexity=100, metric='euclidean',random_state=42,learning_rate=1500)
    # 对数据进行降维
    data_tsne = tsne.fit_transform(data)

    # 创建一个新的DataFrame包含降维后的数据
    tsne_df = pd.DataFrame(data_tsne, columns=["Dimension 1", "Dimension 2"])
    tsne_df["Label"] = labels
    # 不需要保留原始域名列，因为 t-SNE 主要关注数据的分布和相似性，而不是域名本身

    # 可视化
    plt.figure(figsize=(8, 6))
    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']

    # 根据标签着色数据点
    for i, label in enumerate(df['label'].unique()):
        plt.scatter(tsne_df.loc[tsne_df['Label'] == label, 'Dimension 1'],
                tsne_df.loc[tsne_df['Label'] == label, 'Dimension 2'],
                c=colors[i],
                label=label)

    plt.title('t-SNE Visualization')
    # 二维投影
    # 生成的二维散点图反映了原始高维数据点之间的相似性和距离关系
    # 保留数据点之间的相似性关系
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.legend()
    plt.show()

    """
    降维后使用DBSCAN进行聚类
    
    """
    # 初始化DBSCAN模型
    dbscan = DBSCAN(eps=2 , min_samples=8)
    # 对降维后的数据进行DBSCAN聚类
    labels_p = dbscan.fit_predict(data_tsne)
    # 将聚类结果添加到原始数据中

    data["Cluster"] = labels_p
    # 绘制散点图，每个聚类用不同颜色表示
    plt.scatter(data_tsne[:, 0], data_tsne[:, 1], c=labels_p)
    plt.title("t-SNE with DBSCAN Clustering")
    plt.xlabel("Dimension 1")
    plt.ylabel("Dimension 2")
    plt.show()
    # 返回降维后的结果
    return data_tsne


def traindataset_tSNE_cluster(csv_file_path):
    # 加载CSV文件
    # 读取训练集数据
    # global data_tsne  
    df = pd.read_csv(csv_file_path)
    
    # /home/dym/恶意域名检测/代码/web_dga/data/features/raw_test_features.csv
    # 提取特征列
    data = df.iloc[:, 2:]  # 假设前两列是标签和域名，所以从第3列开始提取特征
    # 假设标签存储在 'label' 列中
    labels = df["label"]

    

    # 创建一个t-SNE模型
    # n_components：指定要降维到的目标维度数。
    # random_state：这个参数用于控制随机性。
    # perplexity：高维空间中每个对象的有效邻居数量
    # distance metric：距离度量，'euclidean' 欧氏距离
    tsne = TSNE(n_components=2, perplexity=100, metric='euclidean',random_state=42,learning_rate=1500)
    # 对数据进行降维
    data_tsne = tsne.fit_transform(data)

    # 创建一个新的DataFrame包含降维后的数据
    tsne_df = pd.DataFrame(data_tsne, columns=["Dimension 1", "Dimension 2"])
    tsne_df["Label"] = labels
    # 不需要保留原始域名列，因为 t-SNE 主要关注数据的分布和相似性，而不是域名本身

    # 可视化
    plt.figure(figsize=(8, 6))
    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']

    # 根据标签着色数据点
    for i, label in enumerate(df['label'].unique()):
        plt.scatter(tsne_df.loc[tsne_df['Label'] == label, 'Dimension 1'],
                tsne_df.loc[tsne_df['Label'] == label, 'Dimension 2'],
                c=colors[i],
                label=label)
    
    # 标记'Unknown'标签的数据点
    unknown_mask = labels == 'Unknown'
    unknown_data = data_tsne[unknown_mask]
    plt.scatter(unknown_data[:, 0], unknown_data[:, 1], c='gray', marker='x', label='Unknown')




    plt.title('t-SNE Visualization')
    # 二维投影
    # 生成的二维散点图反映了原始高维数据点之间的相似性和距离关系
    # 保留数据点之间的相似性关系
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.legend()
    plt.show()

    """
    降维后使用DBSCAN进行聚类
    
    """
    # 初始化DBSCAN模型
    dbscan = DBSCAN(eps=10 , min_samples=5)
    # 对降维后的数据进行DBSCAN聚类
    labels_p = dbscan.fit_predict(data_tsne[~unknown_mask])
    # 将聚类结果添加到原始数据中

     # 将聚类结果添加到原始数据中
    data.loc[~unknown_mask, "Cluster"] = labels_p
    
    # 绘制散点图，每个聚类用不同颜色表示
    plt.scatter(data_tsne[~unknown_mask, 0], data_tsne[~unknown_mask, 1], c=labels_p)
    plt.title("t-SNE with DBSCAN Clustering")
    plt.xlabel("Dimension 1")
    plt.ylabel("Dimension 2")
    plt.show()
    # 返回降维后的结果
    return data_tsne[~unknown_mask],data_tsne[unknown_mask]


"""
UMAP进行特征降维

"""
def UMAP(csv_file_path):
    # 加载CSV文件
    # 读取训练集数据
    # global data_umap
    # 存储的特征都已经归一化过了 
    # data/features\raw_test_features_2family.csv
    # df = pd.read_csv('./data/features/raw_test_features_1family.csv')
    df = pd.read_csv(csv_file_path)
    # 提取特征列
    data = df.iloc[:, 2:]  # 假设前两列是标签和域名，所以从第3列开始提取特征
    # 假设标签存储在 'label' 列中
    labels = df["label"]
    # 将字符串标签映射为数字
    label_encoder = LabelEncoder()
    labels_numeric = label_encoder.fit_transform(labels)
    # 创建UMAP模型
    umap_model = umap.UMAP(n_neighbors=60, n_components=2, min_dist=0.2,random_state=42)
    # 在降维模型上拟合数据
    # umap_result = umap_model.fit_transform(data)
    data_umap = umap_model.fit_transform(data)
    # 使用散点图可视化降维后的数据，用颜色表示不同标签
    # 创建一个新的DataFrame包含降维后的数据
    UAMP_df = pd.DataFrame(data_umap, columns=["Dimension 1", "Dimension 2"])
    UAMP_df["Label"] = labels
    # 不需要保留原始域名列，因为 t-SNE 主要关注数据的分布和相似性，而不是域名本身

    # data_tsne_with_labels = np.column_stack((data_tsne, labels.values))
    plt.figure(figsize=(10, 8))
    # plt.scatter(umap_result[:, 0], umap_result[:, 1], c=labels, cmap='Spectral', s=5)
    
    plt.scatter(data_umap[:, 0], data_umap[:, 1], c=labels_numeric, cmap='Spectral', s=5)
    plt.gca().set_aspect('equal', 'datalim')
    plt.title('UMAP Projection of DGA Domains')
    plt.savefig('UMAP.png')
    plt.show()
    return UAMP_df

def traindataset_UMAP(csv_file_path):
    # 加载CSV文件
    # 读取训练集数据
    # global data_umap
    # 存储的特征都已经归一化过了 
    # data/features\raw_test_features_2family.csv
    # df = pd.read_csv('./data/features/raw_test_features_1family.csv')
    df = pd.read_csv(csv_file_path)
    # 提取特征列
    data = df.iloc[:, 2:]  # 假设前两列是标签和域名，所以从第3列开始提取特征
    # 假设标签存储在 'label' 列中
    labels = df["label"]
    # 将字符串标签映射为数字
    label_encoder = LabelEncoder()
    labels_numeric = label_encoder.fit_transform(labels)
    # 创建UMAP模型
    umap_model = umap.UMAP(n_neighbors=60, n_components=2, min_dist=0.2,random_state=42)
    # 在降维模型上拟合数据
    # umap_result = umap_model.fit_transform(data)
    data_umap = umap_model.fit_transform(data)

    # 交换生成的维度的顺序
    data_umap_swapped = np.column_stack((data_umap[:, 1], data_umap[:, 0]))
    # 使用散点图可视化降维后的数据，用颜色表示不同标签
    plt.figure(figsize=(10, 8))
    # plt.scatter(umap_result[:, 0], umap_result[:, 1], c=labels, cmap='Spectral', s=5)
    
    plt.scatter(data_umap[:, 0], data_umap[:, 1], c=labels_numeric, cmap='Spectral', s=5)
    plt.gca().set_aspect('equal', 'datalim')
    plt.title('UMAP Projection of DGA Domains')
    plt.show()

    """
    降维后使用DBSCAN进行聚类
    
    """
    # 初始化DBSCAN模型
    dbscan = DBSCAN(eps=0.5, min_samples=8)
    # 对降维后的数据进行DBSCAN聚类
    # labels_p = dbscan.fit_predict(umap_result)
    labels_p = dbscan.fit_predict(data_umap)
    # 将聚类结果添加到原始数据中

    data["Cluster"] = labels_p
    # 绘制散点图，每个聚类用不同颜色表示
    # plt.scatter(umap_result[:, 0], umap_result[:, 1], c=labels_p)
    plt.scatter(data_umap[:, 0], data_umap[:, 1], c=labels_p)
    plt.title(" UMAP with DBSCAN Clustering")
    plt.xlabel("Dimension 1")
    plt.ylabel("Dimension 2")
    plt.show()
    # 返回降维后的UMAP结果
    return data_umap


def calculate_p_value(D, A, epsilon, z):
    """
    输入：
    一组对象的集合D，z1到zn-1
    非一致度量函数A(KNN和KDE)
    两种方法后续再补充进来
    显著水平
    新对象z(也就是每个格点)
    输出：
    p_value
    布尔值表示是否与训练对象一致
    """
    # 初始化P-Value pn
    pn = 0.0

    # 步骤1：设置zn为新对象z，并扩展D
    D = np.vstack([D, z])
    # 步骤2：计算每个对象zi与其他对象的不一致性度量αi
    # alphas 是一个空列表，用于存储每个对象zi与其他对象的不一致性度量αi
    alphas = []
    print(len(D)) # 增加了一个点应该是316个点了

    for i in range(len(D)):
        # 移除对象zi并计算αi
        D_minus_i = np.delete(D, i, axis=0)
        # 衡量不一致性
        # 这里有两个方法，直接用A的值来判断
        if(A==0):
            alpha_i = k_nn_nonconformity_measure(D_minus_i, D[i],1)
        else:
            alpha_i = kde_nonconformity_measure(D_minus_i, D[i],0.1)
            
        # 记录了训练对象中每个对象与其他对象的不一致性度量
        # print(f"alpha_i = {alpha_i}")
        alphas.append(alpha_i)
    
    # 步骤3：生成随机数τ，介于0和1之间的随机值
    tau = np.random.uniform(0, 1)

    # 步骤4：计算P-Value pn
    alpha_n = alphas[-1]  # 新对象z的不一致性度量αn
    # 满足 αi > αn 的i的数量
    num_alpha_gt_alpha_n = np.sum(np.array(alphas) > alpha_n)
    # 满足 αi = αn 且 i < n 的i的数量，其中i < n表示当前对象zi之前的对象
    num_alpha_eq_alpha_n_and_i_lt_n = np.sum((np.array(alphas) == alpha_n) & (np.arange(len(D)) < len(D) - 1))
    # 根据公式计算p_value
    pn = (num_alpha_gt_alpha_n + num_alpha_eq_alpha_n_and_i_lt_n * tau) / len(D)
    
    # 步骤5：判断是否与训练对象一致
    is_consistent = pn > epsilon
    
    return pn, is_consistent


""""
Kernel PCA 降维
"""
def traindataset_KPCA(csv_file_path):
    df = pd.read_csv(csv_file_path)
    # 提取特征列
    data = df.iloc[:, 2:]  # 假设前两列是标签和域名，所以从第3列开始提取特征
    # 假设标签存储在 'label' 列中
    labels = df["label"]
    # 使用 Kernel PCA 进行降维
    kpca = KernelPCA(n_components=2, kernel='rbf')  # 选择高斯核
    features_kpca = kpca.fit_transform(data)
    # 创建一个新的DataFrame包含降维后的数据
    kpca_df = pd.DataFrame(features_kpca, columns=["Dimension 1", "Dimension 2"])
    kpca_df["Label"] = labels
    # 不需要保留原始域名列，因为 t-SNE 主要关注数据的分布和相似性，而不是域名本身

    # 可视化
    plt.figure(figsize=(8, 6))
    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']

    # 根据标签着色数据点
    for i, label in enumerate(df['label'].unique()):
        plt.scatter(kpca_df.loc[kpca_df['Label'] == label, 'Dimension 1'],
                kpca_df.loc[kpca_df['Label'] == label, 'Dimension 2'],
                c=colors[i],
                label=label)

    plt.title('t-SNE Visualization')
    # 二维投影
    # 生成的二维散点图反映了原始高维数据点之间的相似性和距离关系
    # 保留数据点之间的相似性关系
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.legend()
    plt.show()

    """
    降维后使用DBSCAN进行聚类
    
    """
    # 初始化DBSCAN模型
    dbscan = DBSCAN(eps=0.01, min_samples=10)
    # 对降维后的数据进行DBSCAN聚类
    labels_p = dbscan.fit_predict(features_kpca)
    # 将聚类结果添加到原始数据中

    data["Cluster"] = labels_p
    # 绘制散点图，每个聚类用不同颜色表示
    plt.scatter(features_kpca[:, 0], features_kpca[:, 1], c=labels_p)
    plt.title("t-SNE with DBSCAN Clustering")
    plt.xlabel("Dimension 1")
    plt.ylabel("Dimension 2")
    plt.show()
    # 返回降维后的结果
    return features_kpca



# 非一致性度量的两个函数
# 基于k-最近邻（k-NN）的非一致性度量
def k_nn_nonconformity_measure(D, zi, k):
    """
    训练对象集合D和新对象z
    k（邻居数量）
    计算非一致性度量Ai
    """
    # 计算对象zi与其他对象的距离
    dist = np.linalg.norm(D - zi, axis=1)
    # 对距离进行排序，找到第k个最近邻的距离
    k_nearest_distances = np.partition(dist, k)[:k]
    # 计算非一致性度量Ai
    Ai = np.sum(k_nearest_distances)
    return Ai

# 基于核密度估计（KDE）的非一致性度量函数
def kde_nonconformity_measure(D, zi, bandwidth):
    """
    训练对象集合D
    新对象zi，
    以及核带宽（bandwidth），不知道取多少合适
    计算非一致性度量Ai
    """
    # 初始化核密度估计器
    kde = KernelDensity(bandwidth=bandwidth, kernel='gaussian')
    # 拟合核密度估计器到训练对象集合D
    kde.fit(D)
    # 计算新对象zi的核密度分数
    log_density = kde.score_samples(zi.reshape(1, -1))
    # 计算非一致性度量Ai
    Ai = -log_density
    return Ai

def CP_cluster():
    # 步骤1：创建d维格点网格
    # 因为已经降维过了，所以此时维度=2
    # features是不是应该指向降维后的特征点，(n_samples, n_features),(对象数，2（已经降维过的）)
    # 应该就是降维后的data_tsne
    features= data_tsne_global
    # features= data_umap_global
    print(features.shape)
    # 获取数据的形状
    num_objects, num_features = features.shape
    # 每个特征的值范围内，有多少个格点要生成，需要后续再调整
    num_points_per_dimension=1000
    d = 2  # 特征的数量
    print(d)
    # 存储每个特征的格点
    
    all_grid_points = []
    # 特征应该只有两个
    # 特征应该只有两个
    for feature_index in range(num_features):
        feature = features[:, feature_index]  # 获取一个特征的数据
        # 对于每个特征，分别计算该特征的最小值和最大值。确定特征值的范围。
        
        # 计算该特征的最小值和最大值
        min_val, max_val = np.min(feature), np.max(feature)
        
        print(min_val)
        print(max_val)
        
        # 将特征值范围分成若干等间距的格点
        num_points_per_dimension = int((max_val - min_val) / 0.5)  # 10是一个可以调整的参数，表示每个特征上的格点数量
        print(num_points_per_dimension)
        # np.linspace() 函数，创建一个在最小值和最大值之间等间距分布的一维数组
        grid = np.linspace(min_val, max_val, num=num_points_per_dimension)
        all_grid_points.append(grid)
    # 使用np.meshgrid函数将 grid_points 列表中的所有特征的格点组合成一个d维格点网格
    """grid_coordinates = np.meshgrid(*grid_points)
    grid_coordinates = np.stack(grid_coordinates, axis=-1)
    print(grid_coordinates.shape)"""
    # 生成格点网络
    grid_points = np.array(np.meshgrid(*all_grid_points)).T.reshape(-1, num_features)
    grid_cell_distance_threshold =  (max_val - min_val) / num_points_per_dimension
    # 可视化一下
    # 可视化结果
    plt.figure(figsize=(8, 8))

    # 绘制格子线
    for i in range(num_features):
        for point in all_grid_points[i]:
            plt.axvline(point, color='gray', linestyle='--', linewidth=0.5)

    # 绘制格点
    plt.plot(grid_points[:, 0], grid_points[:, 1], 'rx', markersize=8)  # 使用"x"形状，设置标记大小

    # 设置坐标轴标签
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')

    """plt.scatter(grid_coordinates[:, 0], grid_coordinates[:, 1], marker='o', s=10)
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.title('2D Grid Points')
    plt.grid(True)
    plt.show()  """
    # 设置图标题
    plt.title('2D Grid Points')

    # 显示格子图
    plt.grid(True)
    plt.show()


    # 步骤2：计算P-Values
    # 存储每个格点的p_value
    p_values = []
    # 遍历了经过整形的 grid_coordinates 
    # grid_coordinates 本来是一个 d 维网格，但在这里通过 reshape(-1, d) 被重新整形成一个一维数组，以便逐个访问每个格点。
    
    for grid_point in grid_points.reshape(-1, d):
        # 使用CP算法计算P-Value，将grid_point视为新对象z
        # 一组对象的集合D可以直接用data_tsne表示吗
        # calculate_p_value()有四个参数：data_tsne、A(准备直接用整数来指示两个不同函数)、epsilon（暂时取0.05？）、grid_point
        print(f"grid_point = {grid_point}")
        p_value = calculate_p_value(data_tsne_global,0,0.05,grid_point)
        # p_value = calculate_p_value(data_umap_global,0,0.05,grid_point)
        print(p_value) # p_value也算出来了，但是好像死循环了
        p_values.append(p_value)
    
    # 步骤3：一致性预测
    # 需要可视化一致性预测结果

    epsilon = 0.05  # 显著性值ε,出错率，可以修改进行对比
    # consistent_points,存储一致性预测为一致的格点
    # grid_coordinates.reshape(-1, d)将之前创建的一维格点坐标重新整形为一个 d 维数组，然后使用 zip 函数将格点坐标和对应的 P-Values 进行配对
    # zip(grid_coordinates.reshape(-1, d), p_values)是将格点坐标和对应的P-Value（概率值）一一对应起来，这样每个格点都有一个对应的P-Value。

    # 一致性的格点，小于设定的最低阈值就不显示颜色了
    consistent_points = [
    point for point, (p_value, is_consistent) in zip(grid_points.reshape(-1, d), p_values) 
    if is_consistent]
    # 解压缩 x 和 y 坐标
    x_coords, y_coords = zip(*consistent_points)

    # 提取 p_value 值，较大的 p_value 对应较深的颜色
   
    p_values_for_color = [p_value for p_value, is_consistent in p_values if is_consistent]
    
    # 创建一个散点图来可视化一致性预测结果，根据 p_value 的大小设置颜色深浅
    """plt.figure(figsize=(10, 8))
    plt.scatter(x_coords, y_coords, s=50, c=p_values_for_color, cmap='viridis', edgecolors='k')
    plt.colorbar(label='Color Intensity (p_value)')
    plt.title('Consistency Prediction Results')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.show() 
    """
    # 调整颜色映射，使用更细致的红色渐变
    cmap = plt.cm.get_cmap('Reds', len(p_values_for_color))

    # 创建一个散点图来可视化一致性预测结果
    plt.figure(figsize=(10, 8))

    # 绘制第一张图（未添加新样本点）
    plt.subplot(1, 2, 1)
    plt.scatter(x_coords, y_coords, s=100, c=p_values_for_color, cmap=cmap, edgecolors='k', marker='x', linewidth=0.5)

    # 添加颜色条，用于表示颜色和 p 值之间的对应关系
    # cbar = plt.colorbar(ticks=np.linspace(0, 1, len(p_values_for_color)), label='Color Intensity (p_value)')
    # cbar.set_ticklabels([f'{p:.2f}' for p in p_values_for_color])
    

    # 设置图表标题和坐标轴标签
    plt.title('Consistency Prediction Results')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')

    # 显示网格线
    plt.grid(True, linestyle='--', alpha=0.7)

    # 添加颜色条
    cbar = plt.colorbar(ticks=np.linspace(0.2, 0.8, 10), label='Color Intensity (p_value)')

    # 绘制第二张图（添加了新样本点）
    plt.subplot(1, 2, 2)
    plt.scatter(x_coords, y_coords, s=100, c=p_values_for_color, cmap=cmap, edgecolors='k', marker='x', linewidth=0.5, label='Existing Malicious Domains')
    plt.scatter(data_tsne_global2[:, 0], data_tsne_global2[:, 1], c='blue', marker='o', label='New Malicious Domains')
    plt.title('Consistency Prediction Results (After Adding New Samples)')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend()  # 添加图例，显示新的点的标签

    # 显示整体图
    plt.tight_layout()
    plt.show()


         
    #步骤4：使用邻近规则进行聚类，这里先暂时选用DBSCAN
    X = np.array(consistent_points)
    distance_matrix = pairwise_distances(X, metric='euclidean')
    db = DBSCAN(eps=10, min_samples=5).fit(distance_matrix)
    
    # 绘制聚类结果
    labels = db.labels_
    unique_labels = set(labels)

    # 创建一个颜色映射，为每个簇分配不同的颜色
    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))
    plt.figure(figsize=(10, 8))

    for label, color in zip(unique_labels, colors):
        cluster_mask = labels == label  # 根据聚类标签创建一个布尔掩码
        xy = X[cluster_mask]  # 选择属于当前聚类的点
        plt.scatter(xy[:, 0], xy[:, 1], s=50, c=color, label=f'Cluster {label}', edgecolors='k')

    plt.title('DBSCAN Clustering Results')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.legend()

    plt.show()

def CP_cluster_Unknown():
    # 步骤1：创建d维格点网格
    # 因为已经降维过了，所以此时维度=2
    # features是不是应该指向降维后的特征点，(n_samples, n_features),(对象数，2（已经降维过的）)
    # 应该就是降维后的data_tsne
    features= data_tsne_global
    # features= data_umap_global
    print(features.shape)
    # 获取数据的形状
    num_objects, num_features = features.shape
    # 每个特征的值范围内，有多少个格点要生成，需要后续再调整
    num_points_per_dimension=1000
    d = 2  # 特征的数量
    print(d)
    # 存储每个特征的格点
    
    all_grid_points = []
    # 特征应该只有两个
    # 特征应该只有两个
    for feature_index in range(num_features):
        feature = features[:, feature_index]  # 获取一个特征的数据
        # 对于每个特征，分别计算该特征的最小值和最大值。确定特征值的范围。
        
        # 计算该特征的最小值和最大值
        min_val, max_val = np.min(feature), np.max(feature)
        
        print(min_val)
        print(max_val)
        
        # 将特征值范围分成若干等间距的格点
        num_points_per_dimension = int((max_val - min_val) / 0.5)  # 10是一个可以调整的参数，表示每个特征上的格点数量
        print(num_points_per_dimension)
        # np.linspace() 函数，创建一个在最小值和最大值之间等间距分布的一维数组
        grid = np.linspace(min_val, max_val, num=num_points_per_dimension)
        all_grid_points.append(grid)
    # 使用np.meshgrid函数将 grid_points 列表中的所有特征的格点组合成一个d维格点网格
    grid_cell_distance_threshold =  (max_val - min_val) / num_points_per_dimension
    # 生成格点网络
    grid_points = np.array(np.meshgrid(*all_grid_points)).T.reshape(-1, num_features)
        
    # 可视化结果
    plt.figure(figsize=(8, 8))

    # 绘制格子线
    for i in range(num_features):
        for point in all_grid_points[i]:
            plt.axvline(point, color='gray', linestyle='--', linewidth=0.5)

    # 绘制格点
    plt.plot(grid_points[:, 0], grid_points[:, 1], 'rx', markersize=8)  # 使用"x"形状，设置标记大小

    # 设置坐标轴标签
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')

    # 设置图标题
    plt.title('2D Grid Points')

    # 显示格子图
    plt.grid(True)
    plt.show()


    # 步骤2：计算P-Values
    # 存储每个格点的p_value
    p_values = []
    # 遍历了经过整形的 grid_coordinates 
    # grid_coordinates 本来是一个 d 维网格，但在这里通过 reshape(-1, d) 被重新整形成一个一维数组，以便逐个访问每个格点。
    
    for grid_point in grid_points.reshape(-1, d):
        # 使用CP算法计算P-Value，将grid_point视为新对象z
        # 一组对象的集合D可以直接用data_tsne表示吗
        # calculate_p_value()有四个参数：data_tsne、A(准备直接用整数来指示两个不同函数)、epsilon（暂时取0.05？）、grid_point
        print(f"grid_point = {grid_point}")
        p_value = calculate_p_value(data_tsne_global,0,0.05,grid_point)
        # p_value = calculate_p_value(data_umap_global,0,0.05,grid_point)
        print(p_value) # p_value也算出来了，但是好像死循环了
        p_values.append(p_value)
    
    # 步骤3：一致性预测
    # 需要可视化一致性预测结果

    epsilon = 0.05  # 显著性值ε,出错率，可以修改进行对比
    # consistent_points,存储一致性预测为一致的格点
    # grid_coordinates.reshape(-1, d)将之前创建的一维格点坐标重新整形为一个 d 维数组，然后使用 zip 函数将格点坐标和对应的 P-Values 进行配对
    # zip(grid_coordinates.reshape(-1, d), p_values)是将格点坐标和对应的P-Value（概率值）一一对应起来，这样每个格点都有一个对应的P-Value。

    # 一致性的格点，小于设定的最低阈值就不显示颜色了
    consistent_points = [
    point for point, (p_value, is_consistent) in zip(grid_points.reshape(-1, d), p_values) 
    if is_consistent]
    # 解压缩 x 和 y 坐标
    x_coords, y_coords = zip(*consistent_points)

    # 提取 p_value 值，较大的 p_value 对应较深的颜色
   
    p_values_for_color = [p_value for p_value, is_consistent in p_values if is_consistent]
    
    # 创建一个散点图来可视化一致性预测结果，根据 p_value 的大小设置颜色深浅

    # 调整颜色映射，使用更细致的红色渐变
    cmap = plt.cm.get_cmap('Reds', len(p_values_for_color))

    # 创建一个散点图来可视化一致性预测结果
    plt.figure(figsize=(10, 8))

    # 绘制第一张图（未添加新样本点）
    
    plt.scatter(x_coords, y_coords, s=100, c=p_values_for_color, cmap=cmap, edgecolors='k', marker='x', linewidth=0.5)

    # 添加颜色条，用于表示颜色和 p 值之间的对应关系
    # cbar = plt.colorbar(ticks=np.linspace(0, 1, len(p_values_for_color)), label='Color Intensity (p_value)')
    # cbar.set_ticklabels([f'{p:.2f}' for p in p_values_for_color])
    

    # 设置图表标题和坐标轴标签
    plt.title('Consistency Prediction Results')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')

    # 显示网格线
    plt.grid(True, linestyle='--', alpha=0.7)

    # 添加颜色条
    cbar = plt.colorbar(ticks=np.linspace(0.2, 0.8, 10), label='Color Intensity (p_value)')


    # 显示整体图
    
    plt.show()


         
    #步骤4：聚类，这里先暂时选用DBSCAN
    X = np.array(consistent_points)
    distance_matrix = pairwise_distances(X, metric='euclidean')
    db = DBSCAN(eps=10, min_samples=5).fit(distance_matrix)
    
    # 绘制聚类结果
    labels = db.labels_
    unique_labels = set(labels)

    # 创建一个颜色映射，为每个簇分配不同的颜色
    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))
    plt.figure(figsize=(10, 8))

    for label, color in zip(unique_labels, colors):
        cluster_mask = labels == label  # 根据聚类标签创建一个布尔掩码
        xy = X[cluster_mask]  # 选择属于当前聚类的点
        plt.scatter(xy[:, 0], xy[:, 1], s=50, c=color, label=f'Cluster {label}', edgecolors='k')

    plt.title('DBSCAN Clustering Results')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.legend()

    plt.show()

    # 步骤五，测试点根据邻近规则聚类
    # 论文中：如果一个测试对象与某个聚类中的某一个点的距离小于网格单元的举例，那么它就被认为属于该聚类
    # 如果一个点与多个聚类相关联，那么这些聚类将被合并
    # 不知道还用不用np.array
    # 初始化一个字典来追踪每个测试对象所属的所有聚类
    
    test_objects = np.array(data_tsne_Unknown) 
    test_objects_clusters = {i: [] for i in range(len(test_objects))}
   
    # 遍历每个聚类
    for label, color in zip(unique_labels, colors):
        cluster_mask = labels == label
        cluster_points = X[cluster_mask]

        # 计算每个测试对象与当前聚类中任一点的距离
        distances_to_cluster = pairwise_distances(test_objects, cluster_points, metric='euclidean')
        min_distances = np.min(distances_to_cluster, axis=1)

        # 判断是否属于当前聚类
        belong_to_cluster = min_distances <= grid_cell_distance_threshold
        belong_to_cluster_indices = np.where(belong_to_cluster)[0]

        
        # 更新测试对象的聚类信息
        for idx in belong_to_cluster_indices:
            test_objects_clusters[idx].append(label)

    # 合并有重叠测试对象的聚类
    for test_object, cluster_labels in test_objects_clusters.items():
        if len(cluster_labels) > 1:
            # 找到多个聚类
            combined_label = min(cluster_labels)  # 使用最小的聚类标签作为合并后的标签
            for label in cluster_labels:
                labels[labels == label] = combined_label  # 更新所有这些聚类的标签

    # 更新unique_labels以反映合并后的聚类
    unique_labels = np.unique(labels)
    # 绘制聚类结果
    plt.figure(figsize=(10, 8))
    for label, color in zip(unique_labels, colors):
        cluster_mask = labels == label
        xy = X[cluster_mask]
        plt.scatter(xy[:, 0], xy[:, 1], s=50, label=f'Cluster {label}',  color=color,edgecolors='k')

        # 绘制分配到该聚类的测试对象
        test_cluster_mask = np.array([label in clusters for clusters in test_objects_clusters.values()])
        
        # 选择属于当前聚类的测试对象
        test_cluster_points = test_objects[test_cluster_mask]
        
        # 先绘制彩色圆点
        plt.scatter(test_cluster_points[:, 0], test_cluster_points[:, 1], s=50, color=color, marker='o')
        # 再绘制黑色的“x”标记
        plt.scatter(test_cluster_points[:, 0], test_cluster_points[:, 1], s=50, c='k', marker='x', edgecolors='k')

    # 绘制未分配到任何聚类的测试对象
    not_assigned_mask = np.array([len(clusters) == 0 for clusters in test_objects_clusters.values()])
    not_assigned_points = test_objects[not_assigned_mask]
    plt.scatter(not_assigned_points[:, 0], not_assigned_points[:, 1], s=50, c='gray', marker='x', label='Unassigned Test Object')
    
    plt.title('Clustering Results with Merged Clusters')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.legend()
    plt.show()

"""
新的尝试
计算恶意与良性样本的p_value
难分处的p_value理论上应该小
"""
"""
calculate_only_p_value()函数实现计算单个点的p_value
"""


def calculate_only_p_value(D,A,z):
    """
    D:该类别的所有数据
    A:非一致性度量函数
    z:需要计算p_value的那个点
    """
    # 步骤1：计算每个对象zi与其他对象的不一致性度量αi
    alphas = []
    for i in range(len(D)):
        D_minus_i = np.delete(D, i, axis=0)
        if A == 0:
            alpha_i = k_nn_nonconformity_measure(D_minus_i, D[i], 1)
        else:
            alpha_i = kde_nonconformity_measure(D_minus_i, D[i], 0.1)
        alphas.append(alpha_i)
    # 这里根据Conformal Prediction那个论文 还是使用了随机数τ，介于0和1之间的随机值
    tau = np.random.uniform(0, 1)
    # 步骤4：计算P-Value pn
    alpha_n = alphas[-1]  # 新对象z的不一致性度量αn
    # 满足 αi > αn 的i的数量
    num_alpha_gt_alpha_n = np.sum(np.array(alphas) > alpha_n)
    # 满足 αi = αn  的i的数量
    num_alpha_eq_alpha_n_and_i_lt_n = np.sum(np.array(alphas) == alpha_n)
    # 根据公式计算p_value
    pn = (num_alpha_gt_alpha_n + num_alpha_eq_alpha_n_and_i_lt_n * tau) / len(D)
    # pn = (num_alpha_gt_alpha_n + num_alpha_eq_alpha_n_and_i_lt_n ) / len(D)
    return pn


def calculate_p_values_by_category(tsne_df, A):
    """
    输入：
    DataFrame tsne_df 包含类别信息
    非一致度量函数 A(KNN和KDE)
    输出：
    p_values_by_category 字典，键是类别，值是包含 p-value 的列表
    """
    p_values_and_coordinates_by_category = {}

    # 获取唯一的类别
    unique_categories = tsne_df["Label"].unique()
    for category in unique_categories:
        # 根据类别筛选数据
        category_data = tsne_df[tsne_df["Label"] == category]
        
        # 提取坐标信息
        coordinates = category_data.drop(columns=["Label"]).values
        # 初始化存储 (坐标, p-value) 的列表
        p_values_and_coordinates = []

        for i in range(len(coordinates)):
            # 计算单个点的 p-value
            p_value= calculate_only_p_value(coordinates, A, coordinates[i])

            # 将 (坐标, p-value) 存储到列表中
            p_values_and_coordinates.append((coordinates[i], p_value))

        # 将类别和对应的 (坐标, p-value) 列表存储到字典中
        p_values_and_coordinates_by_category[category] = p_values_and_coordinates

    return p_values_and_coordinates_by_category

def plot_p_values_by_category(p_values_by_category, base_colors, color_depths):
    """
    根据 p_value 绘制不同类别的散点图

    参数:
    - p_values_by_category (dict): 一个字典，包含每个类别的标签、坐标和p_values的字典
    - base_colors (dict): 一个字典，包含每个类别的基础颜色
    - color_depths (list): 颜色深度的列表，与 p_value 相对应

    返回:
    无返回值，直接绘制图表
    """
    plt.figure(figsize=(10, 6))

    for category_label, data_points in p_values_by_category.items():
        # 分离坐标、标签和p_values
        # coordinate 包括 x 和 y 坐标
        coordinates, p_values = zip(*data_points)

        # 根据标签获取基础颜色
        base_color = base_colors[category_label]

        # 根据 p_values 和颜色深度计算颜色
        # 根据 p_value 和颜色深度计算颜色
        # 绘制散点图
        #colors = [np.array(base_color) * (1 - depth) + np.array([1, 1, 1]) * depth for depth in np.linspace(0,1, len(p_values))]
        #colors = [np.array(base_color) * (1 - p_value) + np.array([1, 1, 1]) * p_value for p_value in p_values]
        colors = p_values
        # 调整颜色映射，使用更细致的红色渐变
        if category_label == 1:
            cmap = plt.cm.get_cmap('Reds', len(colors))
        else:
            cmap = plt.cm.get_cmap('Greens', len(colors))
        # 打印长度信息
        print(f"Length of x: {len(np.arange(len(p_values)))}")
        print(f"Length of colors: {len(colors)}")

        # 绘制散点图
        # 将元组转换为 NumPy 数组
        coordinates = np.array(coordinates)
        
        plt.scatter(coordinates[:, 0], coordinates[:, 1], c=colors, cmap=cmap,edgecolors='k', s=30, label=f'Category {category_label}')
        # 自定义颜色映射
        """print(category_label)
        print(base_color)
        cmap = LinearSegmentedColormap.from_list('custom_cmap', [base_color, [1, 1, 1]], N=256)
        norm = Normalize(vmin=0, vmax=1)  # 标准化颜色映射
        plt.colorbar(norm=norm, cmap=cmap, label='P-Value Intensity')
        """
        
    plt.title('P-Value Visualization by Category')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.legend()
    # plt.colorbar(Normalize(vmin=0, vmax=1), label='P-Value Intensity')
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.savefig('P-Value Visualization by Category')
    plt.show()




if __name__ == "__main__":

    dataset_generation()

    """# remaining_data.csv Alureon家族500个
    csv_file_path = './data/features/Banjorifamilytest.csv'
    # 全局变量
    data_tsne_global = traindataset_tSNE(csv_file_path)
    # sampled_data.csv Alureon家族500个
    csv_file_path2 = './data/features/Banjorifamily.csv'
    data_tsne_global2 = traindataset_tSNE(csv_file_path2)
    CP_cluster()
    """
    # 尝试聚类测试点
    
    # 500个Alureon家族+500个Murofetfamily 家族 + 20 个 未知
    # dataset_generation()进行特征提取和归一化，对于20个未知家族域名标签使用Unknown作区分
    # 结果存储在clusterAM.csv
    # dataset_generation()
    # 使用t-SNE 对所有的聚类测试点降维，包括20个未知的，由于降维后也是知道家族标签的，所以将Unknown区分出来
    # 降维后先做聚类尝试，这里聚类不带标签为Uknown的，可视化聚类结果
    """csv_file_path = './Year_Data/2019_500.csv'
    
    data_tsne_global = UMAP(csv_file_path)
    print(data_tsne_global.info())
    print(data_tsne_global.head())
    p_values_by_category = calculate_p_values_by_category(data_tsne_global,0)
    base_colors_example = {
    1: [1, 0, 0],  # 红色
    0: [0, 1, 0],  # 绿色
    }
    color_depths_example = np.linspace(0, 0.5, 50)
    plot_p_values_by_category(p_values_by_category, base_colors_example, color_depths_example)

    # 可以直接修改 traindataset_tSNE 函数，只涉及到聚类部分可视化
    # data_tsne_global, data_tsne_Unknown = traindataset_tSNE_cluster(csv_file_path)

    # 进行预测，预测结束后，使用邻近规则对20个未知域名进行聚类
    # CP_cluster_Unknown()
    """


    
    
    
